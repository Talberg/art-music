 <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8/dist/teachablemachine-image.min.js"></script>
    <!-- <script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/core.js"></script> -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/music_vae.js"></script> -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/1.2.8/tf.min.js"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0"></script>
</head>
<body>

    <div class="" ></div>

    <div>Teachable Machine Image Model</div>
    <div id="webcam-container"></div>
<button type="button" onclick="init()">Start</button>
<form>
<!-- <input type="file" accept="video/*" capture="camera" id="recorder"> -->
<!-- <video id="player" controls></video> -->


<div id="label-container"></div>
<div id="object" > <button type="button" onClick="showObjectEvent() "  > Generate Song</button></div>
<script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

    // the link to your model provided by Teachable Machine export panel
    const URL = "https://teachablemachine.withgoogle.com/models/juTjqqPvw/";

    let model, webcam, labelContainer, maxPredictions,currentPrediction
    let TWINKLE_TWINKLE = {
  notes: [
    {pitch: 60, startTime: 0.0, endTime: 0.5},
    {pitch: 60, startTime: 0.5, endTime: 1.0},
    {pitch: 67, startTime: 1.0, endTime: 1.5},
    {pitch: 67, startTime: 1.5, endTime: 2.0},
    {pitch: 69, startTime: 2.0, endTime: 2.5},
    {pitch: 69, startTime: 2.5, endTime: 3.0},
    {pitch: 67, startTime: 3.0, endTime: 4.0},
    {pitch: 65, startTime: 4.0, endTime: 4.5},
    {pitch: 65, startTime: 4.5, endTime: 5.0},
    {pitch: 64, startTime: 5.0, endTime: 5.5},
    {pitch: 64, startTime: 5.5, endTime: 6.0},
    {pitch: 62, startTime: 6.0, endTime: 6.5},
    {pitch: 62, startTime: 6.5, endTime: 7.0},
    {pitch: 60, startTime: 7.0, endTime: 8.0},  
  ],
  totalTime: 16
};

    // Load the image model and setup the webcam
    async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";
        navigator.mediaDevices.getUserMedia({video: true})

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // or files from your local hard drive
        // Note: the pose library adds "tmImage" object to your window (window.tmImage)
        model = await tmImage.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        
        const flip = true; // whether to flip the webcam
        webcam = new tmImage.Webcam(200, 200, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append elements to the DOM
        document.getElementById("webcam-container").appendChild(webcam.canvas);
        labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }
    }

    async function loop() {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
    }

    // run the webcam image through the image model
    async function predict() {
        // predict can take in an image, video or canvas html element
        const prediction = await model.predict(webcam.canvas);
        for (let i = 0; i < maxPredictions; i++) {
            
            const classPrediction =
                prediction[i].className + ": " + prediction[i].probability.toFixed(2);
            labelContainer.childNodes[i].innerHTML = classPrediction;
        }
       
      
        logCurrent(prediction)
    }
    function logCurrent(object){
       currentPrediction = object
       setObject()
    //    console.log(currentPrediction)

    }

    function setObject(){
        
        let flatPrediction = JSON.stringify(currentPrediction)
        localStorage.setItem('Prediction',flatPrediction)
        // console.log(currentPrediction)
        showObject()
    }

    function showObject(){
        // console.log( localStorage.Prediction)
    }
    function showObjectEvent(e){
        let objectObject = JSON.parse(localStorage.Prediction)
        
        //make the era show up that has the highest % map object here 
        let TopGuess =0
        let Era
        objectObject.map(Guess=>{

            console.log(TopGuess)
            if(Guess.probability>TopGuess){
                TopGuess=Guess.probability
                Era = Guess.className

                console.log(Era)
            }
            else{
                console.log(Era)
            }
            
           
        }
        
        
           
        )
        console.log(music_rnn)
        play()

    }
    let player = new mm.Player();
       let music_vae = new mm.MusicVAE('https://storage.googleapis.com/magentadata/js/checkpoints/music_vae/mel_4bar_small_q2')
       music_vae.initialize();
       music_rnn = new mm.MusicRNN('https://storage.googleapis.com/magentadata/js/checkpoints/music_rnn/basic_rnn');
music_rnn.initialize();


    rnnPlayer = new mm.Player();

    function play() {
  if (rnnPlayer.isPlaying()) {
    rnnPlayer.stop();
    return;
  }
      
  // The model expects a quantized sequence, and ours was unquantized:
  const qns = mm.sequences.quantizeNoteSequence(TWINKLE_TWINKLE, 4);
  music_rnn
  .continueSequence(qns, 100, 1.5)
  .then((sample) => rnnPlayer.start(sample));
}
    //write a function that the currentPosition is logged then restarts the loop
    




</script>

    
</body>
</html>